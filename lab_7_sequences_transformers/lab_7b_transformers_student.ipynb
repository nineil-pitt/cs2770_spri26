{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "069Tts9PW2qc"
   },
   "source": [
    "# Lab 7b: Learn how to work with Transformers\n",
    "\n",
    "**Week 2, Day 5: Attention and Transformers**\n",
    "\n",
    "**By Neuromatch Academy** and adapated by Prof. Nils Murrugarra from Univ of Pittsburgh\n",
    "\n",
    "__Content creators:__ Bikram Khastgir, Rajaswa Patil, Egor Zverev, Kelson Shilling-Scrivo, Alish Dipani, He He\n",
    "\n",
    "__Content reviewers:__ Ezekiel Williams, Melvin Selim Atay, Khalid Almubarak, Lily Cheng, Hadi Vafaei, Kelson Shilling-Scrivo\n",
    "\n",
    "__Content editors:__ Gagana B, Anoop Kulkarni, Spiros Chavlis\n",
    "\n",
    "__Production editors:__ Khalid Almubarak, Gagana B, Spiros Chavlis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "YUxw9qsIW2qd"
   },
   "source": [
    "---\n",
    "# Tutorial Objectives\n",
    "\n",
    "At the end of today's section, you should be able to\n",
    "- Explain the general attention mechanism using keys, queries, values\n",
    "- Name three applications where attention is useful\n",
    "- Explain why Transformer is more efficient than RNN\n",
    "- Implement self-attention in Transformer\n",
    "- Understand the role of position encoding in Transformer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "d9KTTPPiW2qe"
   },
   "source": [
    "---\n",
    "# Setup\n",
    "\n",
    "In this section, we will install, and import libraries, as well as helper functions needed for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {},
    "id": "ACiRgmnDW2qe"
   },
   "outputs": [],
   "source": [
    "# @title Install dependencies\n",
    "# @markdown There may be *errors* and/or *warnings* reported during the installation. However, they are to be ignored.\n",
    "!pip install tensorboard --quiet\n",
    "!pip install -U transformers --quiet\n",
    "!pip install -U datasets --quiet\n",
    "!pip install fasttext --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "id": "RjwZEYUSW2qe"
   },
   "outputs": [],
   "source": [
    "# @title Install and import feedback gadget\n",
    "\n",
    "!pip3 install vibecheck datatops --quiet\n",
    "\n",
    "from vibecheck import DatatopsContentReviewContainer\n",
    "def content_review(notebook_section: str):\n",
    "    return DatatopsContentReviewContainer(\n",
    "        \"\",  # No text prompt\n",
    "        notebook_section,\n",
    "        {\n",
    "            \"url\": \"https://pmyvdlilci.execute-api.us-east-1.amazonaws.com/klab\",\n",
    "            \"name\": \"neuromatch_dl\",\n",
    "            \"user_key\": \"f379rz8y\",\n",
    "        },\n",
    "    ).render()\n",
    "\n",
    "\n",
    "feedback_prefix = \"W2D5_T1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "id": "sstbqhXMW2qe"
   },
   "outputs": [],
   "source": [
    "# @title Set environment variables\n",
    "\n",
    "import os\n",
    "os.environ['TA_CACHE_DIR'] = 'data/'\n",
    "os.environ['NLTK_DATA'] = 'nltk_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {},
    "id": "bhMts49pW2qe",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e4efb9ef-649b-4e0b-e385-5d62f3ec92ef"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "import nltk\n",
    "import torch\n",
    "import random\n",
    "import string\n",
    "import datasets\n",
    "import fasttext\n",
    "import statistics\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pprint import pprint\n",
    "from tqdm.notebook import tqdm\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "from nltk.corpus import brown\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, BertTokenizer, BertModel, BertForMaskedLM\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "id": "vrhZuMVQW2qe"
   },
   "outputs": [],
   "source": [
    "# @title Figure settings\n",
    "import logging\n",
    "logging.getLogger('matplotlib.font_manager').disabled = True\n",
    "\n",
    "import ipywidgets as widgets  # interactive display\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/content-creation/main/nma.mplstyle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "id": "5wtp4dVjW2qf"
   },
   "outputs": [],
   "source": [
    "# @title Download NLTK data (`punkt`, `averaged_perceptron_tagger`, `brown`, `webtext`)\n",
    "\n",
    "\"\"\"\n",
    "NLTK Download:\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('brown')\n",
    "nltk.download('webtext')\n",
    "\"\"\"\n",
    "\n",
    "import os, requests, zipfile\n",
    "\n",
    "os.environ['NLTK_DATA'] = 'nltk_data/'\n",
    "\n",
    "fname = 'nltk_data.zip'\n",
    "url = 'https://osf.io/download/zqw5s/'\n",
    "\n",
    "r = requests.get(url, allow_redirects=True)\n",
    "\n",
    "with open(fname, 'wb') as fd:\n",
    "  fd.write(r.content)\n",
    "\n",
    "with zipfile.ZipFile(fname, 'r') as zip_ref:\n",
    "  zip_ref.extractall('.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "id": "W_6_vlElW2qf"
   },
   "outputs": [],
   "source": [
    "# @title Helper functions\n",
    "global category\n",
    "global brown_wordlist\n",
    "global w2vmodel\n",
    "category = ['editorial', 'fiction', 'government', 'mystery', 'news',\n",
    "                   'religion', 'reviews', 'romance', 'science_fiction']\n",
    "brown_wordlist = list(brown.words(categories=category))\n",
    "\n",
    "def create_word2vec_model(category = 'news', size = 50, sg = 1, min_count = 10):\n",
    "    sentences = brown.sents(categories=category)\n",
    "    with open(\"brown_sentences.txt\", \"w\") as f:\n",
    "        for sentence in sentences:\n",
    "            f.write(\" \".join(sentence) + \"\\n\")\n",
    "        model = fasttext.train_unsupervised('brown_sentences.txt',\n",
    "                                            model='skipgram',\n",
    "                                            dim=size,\n",
    "                                            minCount=min_count)\n",
    "    return model\n",
    "\n",
    "w2vmodel = create_word2vec_model(category)\n",
    "\n",
    "def model_dictionary(model):\n",
    "  print(w2vmodel.words)\n",
    "  return w2vmodel.words\n",
    "\n",
    "def get_embedding(word, model):\n",
    "  if word in model:\n",
    "    return model[word]\n",
    "  else:\n",
    "    print(f' |{word}| not in model dictionary. Try another word')\n",
    "\n",
    "def check_word_in_corpus(word, model):\n",
    "  if word in model:\n",
    "    print('Word present!')\n",
    "    return model[word]\n",
    "  else:\n",
    "    print('Word NOT present!')\n",
    "    return None\n",
    "\n",
    "def get_embeddings(words, model):\n",
    "  embed_list = [get_embedding(word, model) for word in words]\n",
    "  return np.array(embed_list)\n",
    "\n",
    "def similar_by_word(word, model):\n",
    "  return model.get_nearest_neighbors(word)\n",
    "\n",
    "def similar_by_vector(vector, model):\n",
    "  vecs = [w2vmodel[w] for w in w2vmodel.words]\n",
    "  x = cosine_similarity(vecs, [vector])\n",
    "  top = np.argsort(x, axis=0)[::-1].flatten()\n",
    "  return [w2vmodel.words[w] for w in top[:10]]\n",
    "\n",
    "def softmax(x):\n",
    "    f_x = np.exp(x) / np.sum(np.exp(x))\n",
    "    return f_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "id": "e0rIokdQW2qf"
   },
   "outputs": [],
   "source": [
    "# @title Set random seed\n",
    "\n",
    "# @markdown Executing `set_seed(seed=seed)` you are setting the seed\n",
    "\n",
    "# for DL its critical to set the random seed so that students can have a\n",
    "# baseline to compare their results to expected results.\n",
    "# Read more here: https://pytorch.org/docs/stable/notes/randomness.html\n",
    "\n",
    "# Call `set_seed` function in the exercises to ensure reproducibility.\n",
    "import random\n",
    "import torch\n",
    "\n",
    "def set_seed(seed=None, seed_torch=True):\n",
    "  \"\"\"\n",
    "  Handles variability by controlling sources of randomness\n",
    "  through set seed values\n",
    "\n",
    "  Args:\n",
    "    seed: Integer\n",
    "      Set the seed value to given integer.\n",
    "      If no seed, set seed value to random integer in the range 2^32\n",
    "    seed_torch: Bool\n",
    "      Seeds the random number generator for all devices to\n",
    "      offer some guarantees on reproducibility\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  if seed is None:\n",
    "    seed = np.random.choice(2 ** 32)\n",
    "  random.seed(seed)\n",
    "  np.random.seed(seed)\n",
    "  if seed_torch:\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "  print(f'Random seed {seed} has been set.')\n",
    "\n",
    "\n",
    "# In case that `DataLoader` is used\n",
    "def seed_worker(worker_id):\n",
    "  \"\"\"\n",
    "  DataLoader will reseed workers following randomness in\n",
    "  multi-process data loading algorithm.\n",
    "\n",
    "  Args:\n",
    "    worker_id: integer\n",
    "      ID of subprocess to seed. 0 means that\n",
    "      the data will be loaded in the main process\n",
    "      Refer: https://pytorch.org/docs/stable/data.html#data-loading-randomness for more details\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  worker_seed = torch.initial_seed() % 2**32\n",
    "  np.random.seed(worker_seed)\n",
    "  random.seed(worker_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "id": "P1r-3p1eW2qf"
   },
   "outputs": [],
   "source": [
    "# @title Set device (GPU or CPU). Execute `set_device()`\n",
    "# especially if torch modules used.\n",
    "\n",
    "# inform the user if the notebook uses GPU or CPU.\n",
    "\n",
    "def set_device():\n",
    "  \"\"\"\n",
    "  Set the device. CUDA if available, CPU otherwise\n",
    "\n",
    "  Args:\n",
    "    None\n",
    "\n",
    "  Returns:\n",
    "    Nothing\n",
    "  \"\"\"\n",
    "  device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "  if device != \"cuda\":\n",
    "    print(\"WARNING: For this notebook to perform best, \"\n",
    "        \"if possible, in the menu under `Runtime` -> \"\n",
    "        \"`Change runtime type.`  select `GPU` \")\n",
    "  else:\n",
    "    print(\"GPU is enabled in this notebook.\")\n",
    "\n",
    "  return device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {},
    "id": "niJjaBAtW2qf",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e5c548a0-7963-4bc9-a2a6-7a4aa7f8cf7e"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Random seed 2021 has been set.\n",
      "GPU is enabled in this notebook.\n"
     ]
    }
   ],
   "source": [
    "SEED = 2021\n",
    "set_seed(seed=SEED)\n",
    "DEVICE = set_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "FvAZPK_WW2qf"
   },
   "source": [
    "## Load Yelp dataset\n",
    "\n",
    "**Description**:\n",
    "\n",
    "YELP dataset contains a subset of Yelp's businesses/reviews and user data.\n",
    "\n",
    "    1,162,119 tips by 2,189,457 users\n",
    "    Over 1.2 million business attributes like hours, parking, availability, and ambience\n",
    "    Aggregated check-ins over time for each of the 138,876 businesses\n",
    "\n",
    "Each file is composed of a single object type, one JSON-object per-line.\n",
    "For detailed structure, see [here](https://www.yelp.com/dataset/documentation/main)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "id": "SNS3CpRdW2qf"
   },
   "outputs": [],
   "source": [
    "# @title `load_yelp_data` helper function\n",
    "\n",
    "def load_yelp_data(DATASET, tokenizer):\n",
    "  \"\"\"\n",
    "  Load Train and Test sets from the YELP dataset.\n",
    "\n",
    "  Args:\n",
    "    DATASET: datasets.dataset_dict.DatasetDict\n",
    "      Dataset dictionary object containing 'train' and 'test' sets of YELP reviews and sentiment classes\n",
    "    tokenizer: Transformer autotokenizer object\n",
    "      Downloaded vocabulary from bert-base-cased and cache.\n",
    "\n",
    "  Returns:\n",
    "    train_loader: Iterable\n",
    "      Dataloader for the Training set with corresponding batch size\n",
    "    test_loader: Iterable\n",
    "      Dataloader for the Test set with corresponding batch size\n",
    "    max_len: Integer\n",
    "      Input sequence size\n",
    "    vocab_size: Integer\n",
    "      Size of the base vocabulary (without the added tokens).\n",
    "    num_classes: Integer\n",
    "      Number of sentiment class labels\n",
    "  \"\"\"\n",
    "  dataset = DATASET\n",
    "  dataset['train'] = dataset['train'].select(range(10000))\n",
    "  dataset['test'] = dataset['test'].select(range(5000))\n",
    "  dataset = dataset.map(lambda e: tokenizer(e['text'], truncation=True,\n",
    "                                            padding='max_length'), batched=True)\n",
    "  dataset.set_format(type='torch', columns=['input_ids', 'label'])\n",
    "\n",
    "  train_loader = torch.utils.data.DataLoader(dataset['train'], batch_size=32)\n",
    "  test_loader = torch.utils.data.DataLoader(dataset['test'], batch_size=32)\n",
    "\n",
    "  vocab_size = tokenizer.vocab_size\n",
    "  max_len = next(iter(train_loader))['input_ids'].shape[0]\n",
    "  num_classes = next(iter(train_loader))['label'].shape[0]\n",
    "\n",
    "  return train_loader, test_loader, max_len, vocab_size, num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "id": "zzIolg8nW2qf",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "6163d910-745c-477b-8f81-d220f3dbbb15"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "<class 'datasets.dataset_dict.DatasetDict'>\n"
     ]
    }
   ],
   "source": [
    "# @title Download and load the dataset\n",
    "\n",
    "import requests, tarfile\n",
    "\n",
    "os.environ['HF_DATASETS_CACHE'] = 'data/'\n",
    "\n",
    "url = \"https://osf.io/kthjg/download\"\n",
    "fname = \"huggingface.tar.gz\"\n",
    "\n",
    "if not os.path.exists(fname):\n",
    "  print('Dataset is being downloading...')\n",
    "  r = requests.get(url, allow_redirects=True)\n",
    "  with open(fname, 'wb') as fd:\n",
    "    fd.write(r.content)\n",
    "  print('Download is finished.')\n",
    "\n",
    "  with tarfile.open(fname) as ft:\n",
    "    ft.extractall('data/')\n",
    "  print('Files have been extracted.')\n",
    "\n",
    "DATASET = datasets.load_dataset(\"yelp_review_full\",\n",
    "                                download_mode=\"reuse_dataset_if_exists\",\n",
    "                                cache_dir='data/')\n",
    "\n",
    "# If the above produces an error uncomment below:\n",
    "# DATASET = load_dataset(\"yelp_review_full\", ignore_verifications=True)\n",
    "print(type(DATASET))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "WQepzWOZW2qf"
   },
   "source": [
    "### Tokenizer\n",
    "\n",
    "A tokenizer is in charge of preparing the inputs for a model i.e., splitting strings in sub-word token strings, converting tokens strings to ids and back, and encoding/decoding (i.e., tokenizing and converting to integers). There are multiple tokenizer variants. BERT base model (cased) has been used here. BERT is a transformers model pretrained on a large corpus of English data in a self-supervised fashion. Pretrained model on English language using a masked language modeling (MLM) objective. This model is case-sensitive: it differentiates between english and English. For more information, see [here](https://huggingface.co/bert-base-cased)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {},
    "id": "mrIKiaLSW2qf",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 49,
     "referenced_widgets": [
      "61b90e4b4fe34fa9b848e1de41125f36",
      "0679cab88fe549acac005be4c31e750b",
      "5bbafafbd02f4574ae6a3e5ddb7ccf00",
      "97547b09b19b4bd097d537e78451fe8b",
      "df97936762764369becaaa439d9df599",
      "f08f83724e9e44beaed19e44418042b7",
      "28b3888bdb2d4997851d4214636e65fd",
      "c590b8837b6248b787878a9cdf7ad7bc",
      "62f3def14dfe41cbb2de9d269548571a",
      "5b68681e2bb54f9b9b98fa28dc1b2597",
      "3ecc7bb2d82041f7a32296808b9e01e6"
     ]
    },
    "outputId": "7a0d208a-60c7-4efe-973d-3a6f661e7068"
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "61b90e4b4fe34fa9b848e1de41125f36"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-cased', cache_dir='data/')\n",
    "train_loader, test_loader, max_len, vocab_size, num_classes = load_yelp_data(DATASET, tokenizer)\n",
    "\n",
    "pred_text = DATASET['test']['text'][28]\n",
    "actual_label = DATASET['test']['label'][28]\n",
    "batch1 = next(iter(test_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "cellView": "form",
    "execution": {},
    "id": "rZ-Hcfq9W2qf"
   },
   "outputs": [],
   "source": [
    "# @title Helper functions for BERT infilling\n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "def transform_sentence_for_bert(sent, masked_word = \"___\"):\n",
    "  \"\"\"\n",
    "  By default takes a sentence with ___ instead of a masked word.\n",
    "\n",
    "  Args:\n",
    "    sent: String\n",
    "      An input sentence\n",
    "    masked_word: String\n",
    "      Masked part of the sentence\n",
    "\n",
    "  Returns:\n",
    "    str: String\n",
    "      Sentence that could be mapped to BERT\n",
    "  \"\"\"\n",
    "  splitted = sent.split(\"___\")\n",
    "  assert (len(splitted) == 2), \"Missing masked word. Make sure to mark it as ___\"\n",
    "\n",
    "  return '[CLS] ' + splitted[0] + \"[MASK]\" + splitted[1] + ' [SEP]'\n",
    "\n",
    "\n",
    "def parse_text_and_words(raw_line, mask = \"___\"):\n",
    "  \"\"\"\n",
    "  Takes a line that has multiple options for some position in the text.\n",
    "\n",
    "  Usage/Example:\n",
    "    Input: The doctor picked up his/her bag\n",
    "    Output: (The doctor picked up ___ bag, ['his', 'her'])\n",
    "\n",
    "  Args:\n",
    "    raw_line: String\n",
    "      A line aligning with format - 'some text option1/.../optionN some text'\n",
    "    mask: String\n",
    "      The replacement for .../... section\n",
    "\n",
    "  Returns:\n",
    "    str: String\n",
    "      Text with mask instead of .../... section\n",
    "    list: List\n",
    "      List of words from the .../... section\n",
    "  \"\"\"\n",
    "  splitted = raw_line.split(' ')\n",
    "  mask_index = -1\n",
    "  for i in range(len(splitted)):\n",
    "    if \"/\" in splitted[i]:\n",
    "      mask_index = i\n",
    "      break\n",
    "  assert(mask_index != -1), \"No '/'-separated words\"\n",
    "  words = splitted[mask_index].split('/')\n",
    "  splitted[mask_index] = mask\n",
    "  return \" \".join(splitted), words\n",
    "\n",
    "\n",
    "def get_probabilities_of_masked_words(text, words):\n",
    "  \"\"\"\n",
    "  Computes probabilities of each word in the masked section of the text.\n",
    "\n",
    "  Args:\n",
    "    text: String\n",
    "      A sentence with ___ instead of a masked word.\n",
    "    words: List\n",
    "      Array of words.\n",
    "\n",
    "  Returns:\n",
    "    list: List\n",
    "      Predicted probabilities for given words.\n",
    "  \"\"\"\n",
    "  text = transform_sentence_for_bert(text)\n",
    "  tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "  for i in range(len(words)):\n",
    "    words[i] = tokenizer.tokenize(words[i])[0]\n",
    "  words_idx = [tokenizer.convert_tokens_to_ids([word]) for word in words]\n",
    "  tokenized_text = tokenizer.tokenize(text)\n",
    "  indexed_tokens = tokenizer.convert_tokens_to_ids(tokenized_text)\n",
    "  masked_index = tokenized_text.index('[MASK]')\n",
    "  tokens_tensor = torch.tensor([indexed_tokens])\n",
    "\n",
    "  pretrained_masked_model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "  pretrained_masked_model.eval()\n",
    "\n",
    "  # Predict all tokens\n",
    "  with torch.no_grad():\n",
    "    predictions = pretrained_masked_model(tokens_tensor)\n",
    "  probabilities = F.softmax(predictions[0][0,masked_index], dim = 0)\n",
    "  predicted_index = torch.argmax(probabilities).item()\n",
    "\n",
    "  return [probabilities[ix].item() for ix in words_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "9V16Fz9SW2qf"
   },
   "source": [
    "---\n",
    "# Section 1: Attention overview\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "LhehfEmaW2qg"
   },
   "source": [
    "We have seen how RNNs and LSTMs can be used to encode the input and handle long range dependence through recurrence. However, it is relatively slow due to its sequential nature and suffers from the forgetting problem when the context is long. Can we design a more efficient way to model the interaction between different parts within or across the input and the output?\n",
    "\n",
    "Today we will study the attention mechanism and how to use it to represent a sequence, which is at the core of large-scale Transformer models.\n",
    "\n",
    "In a nut shell, attention allows us to represent an object (e.g., a word, an image patch, a sentence) in the context of other objects, thus modeling the relation between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "Kof8LeUHW2qg"
   },
   "source": [
    "---\n",
    "# Section 2: Queries, keys, and values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "JYO1TBhkW2qg"
   },
   "source": [
    "One way to think about attention is to consider a dictionary that contains all information needed for our task. Each entry in the dictionary contains some value and the corresponding key to retrieve it. For a specific prediction, we would like to retrieve relevant information from the dictionary. Therefore, we issue a query, match it to keys in the dictionary, and return the corresponding values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "lOVDXhsgW2qg"
   },
   "source": [
    "### Interactive Demo 2: Intution behind Attention\n",
    "\n",
    "To understand how attention works, let us consider an example of the word 'bank', which has an ambigious meaning dependent upon the context of the sentence. Let the word 'bank' be the query and consider two keys, each with a different meaning of the word 'bank'.\n",
    "\n",
    "Check out the attention scores of different words in the sentences and the words similar to the final value embedding.\n",
    "\n",
    "In this example we use a simplified model of scaled dot-attention with no linear projections and the word2vec model is used to embed the words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {},
    "id": "AatcADT3W2qg",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "af639ab3-1272-4f21-944e-7a2dc5b35276"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Words Similar to Query (bank):\n",
      "1. banks\n",
      "2. cocktail\n",
      "3. horizon\n",
      "4. etc.\n",
      "5. Huff\n",
      "6. ranks\n",
      "7. oil\n",
      "8. pilot\n",
      "9. sugar\n",
      "10. Instead\n",
      "\n",
      "Query: bank, Keys: ['river', 'bank', 'sea', 'water'] \n",
      "\n",
      "\n",
      "Attention Scores: \n",
      " [('river', np.float32(1.89147)), ('bank', np.float32(1.7732699)), ('sea', np.float32(1.7814456)), ('water', np.float32(2.0334795))] \n",
      "\n",
      "\n",
      "Scaled Attention Scores: \n",
      " [('river', np.float64(0.25073529622771684)), ('bank', np.float64(0.24657883816564247)), ('sea', np.float64(0.24686410315422314)), ('water', np.float64(0.2558217624524176))] \n",
      "\n",
      "value: (50,)\n",
      "Words Similar to the final value:\n",
      "1. bottom\n",
      "2. corner\n",
      "3. flew\n",
      "4. narrow\n",
      "5. box\n",
      "6. river\n",
      "7. porch\n",
      "8. Gun\n",
      "9. blanket\n",
      "10. onto\n"
     ]
    }
   ],
   "source": [
    "# @title Enter your own query/keys\n",
    "def get_value_attention(w2vmodel, query, keys):\n",
    "  \"\"\"\n",
    "  Function to compute the scaled dot product\n",
    "\n",
    "  Args:\n",
    "    w2vmodel: nn.Module\n",
    "      Embedding model on which attention scores need to be calculated\n",
    "    query: string\n",
    "      Query string\n",
    "    keys: string\n",
    "      Key string\n",
    "\n",
    "  Returns:\n",
    "    None\n",
    "  \"\"\"\n",
    "  # Get the Word2Vec embedding of the query\n",
    "  query_embedding = get_embedding(query, w2vmodel)\n",
    "  # Print similar words to the query\n",
    "  print(f'Words Similar to Query ({query}):')\n",
    "  query_similar_words = similar_by_word(query, w2vmodel)\n",
    "  for idx in range(len(query_similar_words)):\n",
    "    print(f'{idx+1}. {query_similar_words[idx][1]}')\n",
    "  # Get scaling factor i.e. the embedding size\n",
    "  scale = w2vmodel.get_dimension()\n",
    "  # Get the Word2Vec embeddings of the keys\n",
    "  keys = keys.split(' ')\n",
    "  key_embeddings = get_embeddings(keys, w2vmodel)\n",
    "  # Calculate unscaled attention scores\n",
    "  attention = np.dot(query_embedding , key_embeddings.T )\n",
    "  print(f'\\nQuery: {query}, Keys: {keys} \\n')\n",
    "  print(f'\\nAttention Scores: \\n {list(zip(keys, attention))} \\n')\n",
    "  # Scale the attention scores\n",
    "  scaled_attention =  attention / np.sqrt(scale)\n",
    "  # Normalize the scaled attention scores to calculate the probability distribution\n",
    "  softmax_attention = softmax(scaled_attention)\n",
    "  # Print attention scores\n",
    "  print(f'\\nScaled Attention Scores: \\n {list(zip(keys, softmax_attention))} \\n')\n",
    "  # Calculate the value\n",
    "  value = np.dot(softmax_attention, key_embeddings)\n",
    "  print(f'value: {value.shape}')\n",
    "  # Print words similar to the calculated value\n",
    "  print(f'Words Similar to the final value:')\n",
    "  value_similar_words = similar_by_vector(value, w2vmodel)\n",
    "  for idx in range(len(value_similar_words)):\n",
    "    print(f'{idx+1}. {value_similar_words[idx]}')\n",
    "  return None\n",
    "\n",
    "\n",
    "# w2vmodel model is created in helper functions\n",
    "query = 'bank'  # @param \\['bank']\n",
    "keys = 'river bank sea water'  # @param \\['bank customer need money', 'river bank sea water']\n",
    "get_value_attention(w2vmodel, query, keys)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "v0p2beGDW2qg"
   },
   "source": [
    "Now that you understand how the model works. Feel free to try your own set of queries and keys. Use the cell below to test if a word is present in the corpus. Then enter your query and keys in the cell below.\n",
    "\n",
    "**Note:** be careful with spacing for the keys!\n",
    "\n",
    "There should only be 1 space between each key, and no spaces before or after for the cell to function properly!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "RdEl0KiAW2qj"
   },
   "source": [
    "### Coding Exercise 2: Dot product attention\n",
    "\n",
    "In this exercise, let's compute the scaled dot product attention using its matrix form.\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathrm{softmax} \\left( \\frac{Q K^\\text{T}}{\\sqrt{d}} \\right) V\n",
    "\\end{equation}\n",
    "\n",
    "where $Q$ denotes the query or values of the embeddings (in other words the hidden states), $K$ the key, and $k$ denotes the dimension of the query key vector.\n",
    "\n",
    "The division by square-root of d is to stabilize the gradients.\n",
    "\n",
    "Note: the function takes an additional argument `h` (number of heads). You can assume it is 1 for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "hQzFlDmHW2qj"
   },
   "source": [
    "```python\n",
    "class DotProductAttention(nn.Module):\n",
    "  \"\"\" Scaled dot product attention. \"\"\"\n",
    "\n",
    "  def __init__(self, dropout, **kwargs):\n",
    "    \"\"\"\n",
    "    Constructs a Scaled Dot Product Attention Instance.\n",
    "\n",
    "    Args:\n",
    "      dropout: Integer\n",
    "        Specifies probability of dropout hyperparameter\n",
    "\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    super(DotProductAttention, self).__init__(**kwargs)\n",
    "    self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "  def calculate_score(self, queries, keys):\n",
    "      \"\"\"\n",
    "      Compute the score between queries and keys.\n",
    "\n",
    "      Args:\n",
    "      queries: Tensor\n",
    "        Query is your search tag/Question\n",
    "        Shape of `queries`: (`batch_size`, no. of queries, head,`k`)\n",
    "      keys: Tensor\n",
    "        Descriptions associated with the database for instance\n",
    "        Shape of `keys`: (`batch_size`, no. of key-value pairs, head, `k`)\n",
    "      \"\"\"\n",
    "      return torch.bmm(queries, keys.transpose(1, 2)) / math.sqrt(queries.shape[-1]) # Batch Matrix-matrix product - https://docs.pytorch.org/docs/stable/generated/torch.bmm.html\n",
    "\n",
    "  def forward(self, queries, keys, values, b, h, t, k):\n",
    "    \"\"\"\n",
    "    Compute dot products. This is the same operation for each head,\n",
    "    so we can fold the heads into the batch dimension and use torch.bmm\n",
    "    Note: .contiguous() doesn't change the actual shape of the data,\n",
    "    but it rearranges the tensor in memory, which will help speed up the computation\n",
    "    for this batch matrix multiplication.\n",
    "    .transpose() is used to change the shape of a tensor. It returns a new tensor\n",
    "    that shares the data with the original tensor. It can only swap two dimensions.\n",
    "\n",
    "    Args:\n",
    "      queries: Tensor\n",
    "        Query is your search tag/Question\n",
    "        Shape of `queries`: (`batch_size`, no. of queries, head,`k`)\n",
    "      keys: Tensor\n",
    "        Descriptions associated with the database for instance\n",
    "        Shape of `keys`: (`batch_size`, no. of key-value pairs, head, `k`)\n",
    "      values: Tensor\n",
    "        Values are returned results on the query\n",
    "        Shape of `values`: (`batch_size`, head, no. of key-value pairs,  `k`)\n",
    "      b: Integer\n",
    "        Batch size\n",
    "      h: Integer\n",
    "        Number of heads\n",
    "      t: Integer\n",
    "        Number of keys/queries/values (for simplicity, let's assume they have the same sizes)\n",
    "      k: Integer\n",
    "        Embedding size\n",
    "\n",
    "    Returns:\n",
    "      out: Tensor\n",
    "        Matrix Multiplication between the keys, queries and values.\n",
    "    \"\"\"\n",
    "    keys = keys.transpose(1, 2).contiguous().view(b * h, t, k)\n",
    "    queries = queries.transpose(1, 2).contiguous().view(b * h, t, k)\n",
    "    values = values.transpose(1, 2).contiguous().view(b * h, t, k)\n",
    "\n",
    "    #################################################\n",
    "    ## Implement Scaled dot product attention\n",
    "    # See the shape of the queries and keys above. You may want to use the `transpose` function\n",
    "    raise NotImplementedError(\"Scaled dot product attention `forward`\")\n",
    "    #################################################\n",
    "\n",
    "    # Matrix Multiplication between the keys and queries\n",
    "    score = self.calculate_score(..., ...)  # size: (b * h, t, t)\n",
    "    softmax_weights = F.softmax(..., dim=2)  # row-wise normalization of weights\n",
    "\n",
    "    # Matrix Multiplication between the output of the key and queries multiplication and values.\n",
    "    out = torch.bmm(self.dropout(...), values).view(b, h, t, k)  # rearrange h and t dims\n",
    "    out = out.transpose(1, 2).contiguous().view(b, t, h * k)\n",
    "\n",
    "    return out\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {},
    "id": "pDi7EvdhW2qj"
   },
   "outputs": [],
   "source": [
    "# TODO: add your solution here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {},
    "id": "VIhgrAoKW2qj",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "430c2591-fc62-4108-84a7-135998e79ab2"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Error: DotProductAttention is not implemented.\n"
     ]
    }
   ],
   "source": [
    "# @title Check Coding Exercise 2!\n",
    "\n",
    "try:\n",
    "  # Instantiate dot product attention\n",
    "  dot_product_attention = DotProductAttention(0)\n",
    "\n",
    "  # Encode query, keys, values and answers\n",
    "  queries = torch.Tensor([[[[12., 2., 17., 88.]], [[1., 43., 13., 7.]], [[69., 48., 18, 55.]]]])\n",
    "  keys = torch.Tensor([[[[10., 99., 65., 10.]], [[85., 6., 114., 53.]], [[25., 5., 3, 4.]]]])\n",
    "  values = torch.Tensor([[[[33., 32., 18., 3.]], [[36., 77., 90., 37.]], [[19., 47., 72, 39.]]]])\n",
    "  answer = torch.Tensor([[[36., 77., 90., 37.], [33., 32., 18.,  3.], [36., 77., 90., 37.]]])\n",
    "\n",
    "  b, t, h, k = queries.shape\n",
    "\n",
    "  # Find dot product attention\n",
    "  out = dot_product_attention(queries, keys, values, b, h, t, k)\n",
    "\n",
    "  if torch.equal(out, answer):\n",
    "    print('Correctly implemented!')\n",
    "  else:\n",
    "    print('ERROR!')\n",
    "except NameError:\n",
    "  print(\"Error: DotProductAttention is not implemented.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "otUBTUDTW2qj"
   },
   "source": [
    "---\n",
    "# Section 3: Multihead attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "IFqow2DmW2qj"
   },
   "source": [
    "One powerful idea in Transformer is multi-head attention, which is used to capture different aspects of the dependence among words (e.g., syntactical vs semantic). For more info see [here](https://lilianweng.github.io/lil-log/2018/06/24/attention-attention.html#a-family-of-attention-mechanisms)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "2J46MWklW2qj"
   },
   "source": [
    "### Coding Exercise 3: $Q$, $K$, $V$ attention\n",
    "\n",
    "In self-attention, the queries, keys, and values are all mapped (by linear projection) from the word embeddings. Implement the mapping functions (`to_keys`, `to_queries`, `to_values`) below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "Sb44YyghW2qj"
   },
   "source": [
    "```python\n",
    "class SelfAttention(nn.Module):\n",
    "  \"\"\"  Multi-head self attention layer. \"\"\"\n",
    "\n",
    "  def __init__(self, k, heads=8, dropout=0.1):\n",
    "    \"\"\"\n",
    "    Initiates the following attributes:\n",
    "    to_keys: Transforms input to k x k*heads key vectors\n",
    "    to_queries: Transforms input to k x k*heads query vectors\n",
    "    to_values: Transforms input to k x k*heads value vectors\n",
    "    unify_heads: combines queries, keys and values to a single vector\n",
    "\n",
    "    Args:\n",
    "      k: Integer\n",
    "        Size of attention embeddings\n",
    "      heads: Integer\n",
    "        Number of attention heads\n",
    "\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    self.k, self.heads = k, heads\n",
    "    #################################################\n",
    "    ## Complete the arguments of the Linear mapping\n",
    "    ## The first argument should be the input dimension\n",
    "    # The second argument should be the output dimension\n",
    "    raise NotImplementedError(\"Linear mapping `__init__`\")\n",
    "    #################################################\n",
    "\n",
    "    self.to_keys = nn.Linear(..., ..., bias=False)\n",
    "    self.to_queries = nn.Linear(..., ..., bias=False)\n",
    "    self.to_values = nn.Linear(..., ..., bias=False)\n",
    "    self.unify_heads = nn.Linear(k * heads, k)\n",
    "    self.attention = DotProductAttention(dropout)\n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\"\n",
    "    Implements forward pass of self-attention layer\n",
    "\n",
    "    Args:\n",
    "      x: Tensor\n",
    "        Batch x t x k sized input\n",
    "\n",
    "    Returns:\n",
    "      unify_heads: Tensor\n",
    "        Self-attention based unified Query/Value/Key tensors\n",
    "    \"\"\"\n",
    "    b, t, k = x.size()\n",
    "    h = self.heads\n",
    "\n",
    "    # We reshape the queries, keys and values so that each head has its own dimension\n",
    "    queries = self.to_queries(x).view(b, t, h, k)\n",
    "    keys = self.to_keys(x).view(b, t, h, k)\n",
    "    values = self.to_values(x).view(b, t, h, k)\n",
    "\n",
    "    out = self.attention(queries, keys, values, b, h, t, k)\n",
    "\n",
    "    return self.unify_heads(out)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {},
    "id": "9M6K78U4W2qj"
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "class SelfAttention(nn.Module):\n",
    "  \"\"\"  Multi-head self attention layer. \"\"\"\n",
    "\n",
    "  def __init__(self, k, heads=8, dropout=0.1):\n",
    "    \"\"\"\n",
    "    Initiates the following attributes:\n",
    "    to_keys: Transforms input to k x k*heads key vectors\n",
    "    to_queries: Transforms input to k x k*heads query vectors\n",
    "    to_values: Transforms input to k x k*heads value vectors\n",
    "    unify_heads: combines queries, keys and values to a single vector\n",
    "\n",
    "    Args:\n",
    "      k: Integer\n",
    "        Size of attention embeddings\n",
    "      heads: Integer\n",
    "        Number of attention heads\n",
    "\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    self.k, self.heads = k, heads\n",
    "\n",
    "    self.to_keys = nn.Linear(k, k * heads, bias=False)\n",
    "    self.to_queries = nn.Linear(k, k * heads, bias=False)\n",
    "    self.to_values = nn.Linear(k, k * heads, bias=False)\n",
    "    self.unify_heads = nn.Linear(k * heads, k)\n",
    "\n",
    "    self.attention = DotProductAttention(dropout)\n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\"\n",
    "    Implements forward pass of self-attention layer\n",
    "\n",
    "    Args:\n",
    "      x: Tensor\n",
    "        Batch x t x k sized input\n",
    "\n",
    "    Returns:\n",
    "      unify_heads: Tensor\n",
    "        Self-attention based unified Query/Value/Key tensors\n",
    "    \"\"\"\n",
    "    b, t, k = x.size()\n",
    "    h = self.heads\n",
    "\n",
    "    # We reshape the queries, keys and values so that each head has its own dimension\n",
    "    queries = self.to_queries(x).view(b, t, h, k)\n",
    "    keys = self.to_keys(x).view(b, t, h, k)\n",
    "    values = self.to_values(x).view(b, t, h, k)\n",
    "\n",
    "    out = self.attention(queries, keys, values, b, h, t, k)\n",
    "\n",
    "    return self.unify_heads(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "59MJ684IW2qj"
   },
   "source": [
    "In practice PyTorch's `torch.nn.MultiheadAttention()` function is used.\n",
    "\n",
    "Documentation for the function can be found here: https://pytorch.org/docs/stable/generated/torch.nn.MultiheadAttention.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "PrLhLvcEW2qj"
   },
   "source": [
    "---\n",
    "# Section 4: Transformer overview I\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "wiCO0C2HW2qj"
   },
   "source": [
    "### Coding Exercise 4: Transformer encoder\n",
    "\n",
    "A transformer block consists of three core layers (on top of the input): self attention, layer normalization, and feedforward neural network.\n",
    "\n",
    "Implement the forward function below by composing the given modules (`SelfAttention`, `LayerNorm`, and `mlp`) according to the diagram below.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content-dl/main/tutorials/W2D5_AttentionAndTransformers/static/transformers1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "WEaKdtOdW2qj"
   },
   "source": [
    "```python\n",
    "class TransformerBlock(nn.Module):\n",
    "  \"\"\" Block to instantiate transformers. \"\"\"\n",
    "\n",
    "  def __init__(self, k, heads):\n",
    "    \"\"\"\n",
    "    Initiates following attributes\n",
    "    attention: Initiating Multi-head Self-Attention layer\n",
    "    norm1, norm2: Initiating Layer Norms\n",
    "    mlp: Initiating Feed Forward Neural Network\n",
    "\n",
    "    Args:\n",
    "      k: Integer\n",
    "        Attention embedding size\n",
    "      heads: Integer\n",
    "        Number of self-attention heads\n",
    "\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "    self.attention = SelfAttention(k, heads=heads)\n",
    "\n",
    "    self.norm_1 = nn.LayerNorm(k)\n",
    "    self.norm_2 = nn.LayerNorm(k)\n",
    "\n",
    "    hidden_size = 2 * k  # This is a somewhat arbitrary choice\n",
    "\n",
    "    self.mlp = nn.Sequential(\n",
    "        nn.Linear(k, hidden_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_size, k))\n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\"\n",
    "    Defines the network structure and flow across a subset of transformer blocks\n",
    "\n",
    "    Args:\n",
    "      x: Tensor\n",
    "        Input Sequence to be processed by the network\n",
    "\n",
    "    Returns:\n",
    "      x: Tensor\n",
    "        Input post-processing by add and normalise blocks [See Architectural Block above for visual details]\n",
    "    \"\"\"\n",
    "    attended = self.attention(x)\n",
    "    #################################################\n",
    "    ## Implement the add & norm in the first block\n",
    "    raise NotImplementedError(\"Add & Normalize layer 1 `forward`\")\n",
    "    #################################################\n",
    "    # Complete the input of the first Add & Normalize layer\n",
    "    x = self.norm_1(... + x)\n",
    "    feedforward = self.mlp(x)\n",
    "    #################################################\n",
    "    ## Implement the add & norm in the second block\n",
    "    raise NotImplementedError(\"Add & Normalize layer 2 `forward`\")\n",
    "    #################################################\n",
    "    # Complete the input of the second Add & Normalize layer\n",
    "    x = self.norm_2(...)\n",
    "\n",
    "    return x\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {},
    "id": "6TXv2_O1W2qj"
   },
   "outputs": [],
   "source": [
    "# to_remove solution\n",
    "class TransformerBlock(nn.Module):\n",
    "  \"\"\" Block to instantiate transformers. \"\"\"\n",
    "\n",
    "  def __init__(self, k, heads):\n",
    "    \"\"\"\n",
    "    Initiates following attributes\n",
    "    attention: Initiating Multi-head Self-Attention layer\n",
    "    norm1, norm2: Initiating Layer Norms\n",
    "    mlp: Initiating Feed Forward Neural Network\n",
    "\n",
    "    Args:\n",
    "      k: Integer\n",
    "        Attention embedding size\n",
    "      heads: Integer\n",
    "        Number of self-attention heads\n",
    "\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "\n",
    "    self.attention = SelfAttention(k, heads=heads)\n",
    "\n",
    "    self.norm_1 = nn.LayerNorm(k)\n",
    "    self.norm_2 = nn.LayerNorm(k)\n",
    "\n",
    "    hidden_size = 2 * k  # This is a somewhat arbitrary choice\n",
    "    self.mlp = nn.Sequential(\n",
    "        nn.Linear(k, hidden_size),\n",
    "        nn.ReLU(),\n",
    "        nn.Linear(hidden_size, k))\n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\"\n",
    "    Defines the network structure and flow across a subset of transformer blocks\n",
    "\n",
    "    Args:\n",
    "      x: Tensor\n",
    "        Input Sequence to be processed by the network\n",
    "\n",
    "    Returns:\n",
    "      x: Tensor\n",
    "        Input post-processing by add and normalise blocks [See Architectural Block above for visual details]\n",
    "    \"\"\"\n",
    "    attended = self.attention(x)\n",
    "    # Complete the input of the first Add & Normalize layer\n",
    "    x = self.norm_1(attended + x)\n",
    "\n",
    "    feedforward = self.mlp(x)\n",
    "    # Complete the input of the second Add & Normalize layer\n",
    "    x = self.norm_2(feedforward + x)\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "_Io1QiVBW2qj"
   },
   "source": [
    "In practice PyTorch's `torch.nn.Transformer()` layer is used.\n",
    "\n",
    "Documentation for the function can be found here: [https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "-h-CnahqW2qk"
   },
   "source": [
    "Layer Normalization helps in stabilizing the training of models. More information can be found in this paper: Layer Normalization [arxiv:1607.06450](https://arxiv.org/abs/1607.06450).\n",
    "\n",
    "In practice PyTorch's `torch.nn.LayerNorm()` function is used.\n",
    "\n",
    "Documentation for the function can be found here: [https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html](https://pytorch.org/docs/stable/generated/torch.nn.LayerNorm.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "5eQwc_aYW2qk"
   },
   "source": [
    "---\n",
    "# Section 5: Positional encoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "Ie9xtya6W2qk"
   },
   "source": [
    "Self-attention is concerned with relationship between words and is not sensitive to positions or word orderings.\n",
    "Therefore, we use an additional positional encoding to represent the word orders.\n",
    "\n",
    "There are multiple ways to encode the position. For our purpose to have continuous values of the positions based on binary encoding, let's use the following implementation of deterministic (as opposed to learned) position encoding using sinusoidal functions.\n",
    "\n",
    "\\begin{equation}\n",
    "PE_{(pos,2i)} = sin(pos/10000^{2i/d_{model}})\\\\\n",
    "PE_{(pos,2i+1)}=cos(pos/10000^{2i/d_{model}})\n",
    "\\end{equation}\n",
    "\n",
    "Note that in the `forward` function, the positional embedding (`pe`) is added to the token embeddings (`x`) elementwise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {},
    "id": "AOUOuJQpW2qk"
   },
   "outputs": [],
   "source": [
    "# @title Implement `PositionalEncoding()` function\n",
    "# @markdown Bonus: Go through the code to get familiarised with internal working of Positional Encoding\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "  # Source: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "  \"\"\" Block initiating Positional Encodings \"\"\"\n",
    "\n",
    "  def __init__(self, emb_size, dropout=0.1, max_len=512):\n",
    "    \"\"\"\n",
    "    Constructs positional encodings\n",
    "    Positional Encodings inject some information about the relative or absolute position of the tokens in the sequence.\n",
    "\n",
    "    Args:\n",
    "      emb_size: Integer\n",
    "        Specifies embedding size\n",
    "      dropout: Float\n",
    "        Specifies Dropout probability hyperparameter\n",
    "      max_len: Integer\n",
    "        Specifies maximum sequence length\n",
    "\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    super(PositionalEncoding, self).__init__()\n",
    "    self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "    pe = torch.zeros(max_len, emb_size)\n",
    "    position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "    div_term = torch.exp(torch.arange(0, emb_size, 2).float() * (-np.log(10000.0) / emb_size))\n",
    "\n",
    "    # Each dimension of the positional encoding corresponds to a sinusoid.\n",
    "    # The wavelengths form a geometric progression from 2 to 100002.\n",
    "    # This function is chosen as it's hypothesized that it would allow the model\n",
    "    # to easily learn to attend by relative positions, since for any fixed offset k,\n",
    "    # PEpos + k can be represented as a linear function of PEpos.\n",
    "    pe[:, 0::2] = torch.sin(position * div_term)\n",
    "    pe[:, 1::2] = torch.cos(position * div_term)\n",
    "    pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "    self.register_buffer('pe', pe)\n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\"\n",
    "    Defines network structure\n",
    "\n",
    "    Args:\n",
    "      x: Tensor\n",
    "        Input sequence\n",
    "\n",
    "    Returns:\n",
    "      x: Tensor\n",
    "        Output is of the same shape as input with dropout and positional encodings\n",
    "    \"\"\"\n",
    "    x = x + self.pe[:x.size(0), :]\n",
    "    return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "M2hUbFYvW2qk"
   },
   "source": [
    "More information about positional embeddings can be found from these sources:\n",
    "* Attention is all you need: [Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)\n",
    "* Convolutional Sequence to Sequence Learning: [Gehring et al., 2017](https://arxiv.org/abs/1705.03122)\n",
    "* The Illustrated Transformer: [Jay Alammar](https://jalammar.github.io/illustrated-transformer/)\n",
    "* The Annotated Transformer: [Alexander Rush](http://nlp.seas.harvard.edu/annotated-transformer/#positional-encoding)\n",
    "* Transformers and Multi-Head Attention: [Phillip Lippe](https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial6/Transformers_and_MHAttention.html#Positional-encoding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "XyUCjPqcW2qk"
   },
   "source": [
    "**Bonus:** Look into the importance of word ordering (last part of the video) by going through the paper.\n",
    "\n",
    "Masked Language Modeling and the Distributional Hypothesis: [Order Word Matters Pre-training for Little](https://aclanthology.org/2021.emnlp-main.230/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "1E6TO4QyW2qk"
   },
   "source": [
    "---\n",
    "# Section 6: Training Transformers\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "ZtkHjqa7W2qk"
   },
   "source": [
    "### Coding Exercise 6: Transformer Architecture for classification\n",
    "\n",
    "Let's now put together the Transformer model using the components you implemented above. We will use the model for text classification. Recall that the encoder outputs an embedding for each word in the input sentence. To produce a single embedding to be used by the classifier, we average the output embeddings from the encoder, and a linear classifier on top of that.\n",
    "\n",
    "Compute the mean pooling function below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "WJXDtVH3W2qk"
   },
   "source": [
    "```python\n",
    "class Transformer(nn.Module):\n",
    "  \"\"\" Transformer Encoder network for classification. \"\"\"\n",
    "\n",
    "  def __init__(self, k, heads, depth, seq_length, num_tokens, num_classes):\n",
    "    \"\"\"\n",
    "    Initiates the Transformer Network\n",
    "\n",
    "    Args:\n",
    "      k: Integer\n",
    "        Attention embedding size\n",
    "      heads: Integer\n",
    "        Number of self attention heads\n",
    "      depth: Integer\n",
    "        Number of Transformer Blocks\n",
    "      seq_length: Integer\n",
    "        Length of input sequence\n",
    "      num_tokens: Integer\n",
    "        Size of dictionary\n",
    "      num_classes: Integer\n",
    "        Number of output classes\n",
    "\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "\n",
    "    self.k = k\n",
    "    self.num_tokens = num_tokens\n",
    "    self.token_embedding = nn.Embedding(num_tokens, k)\n",
    "    self.pos_enc = PositionalEncoding(k)\n",
    "\n",
    "    transformer_blocks = []\n",
    "    for i in range(depth):\n",
    "      transformer_blocks.append(TransformerBlock(k=k, heads=heads))\n",
    "\n",
    "    self.transformer_blocks = nn.Sequential(*transformer_blocks)\n",
    "    self.classification_head = nn.Linear(k, num_classes)\n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\"\n",
    "    Forward pass for Classification within Transformer network\n",
    "\n",
    "    Args:\n",
    "      x: Tensor\n",
    "        (b, t) sized tensor of tokenized words\n",
    "\n",
    "    Returns:\n",
    "      logprobs: Tensor\n",
    "        Log-probabilities over classes sized (b, c)\n",
    "    \"\"\"\n",
    "    x = self.token_embedding(x) * np.sqrt(self.k)\n",
    "    x = self.pos_enc(x)\n",
    "    x = self.transformer_blocks(x)\n",
    "\n",
    "    #################################################\n",
    "    ## Implement the Mean pooling to produce\n",
    "    # the sentence embedding\n",
    "    raise NotImplementedError(\"Mean pooling `forward`\")\n",
    "    #################################################\n",
    "    sequence_avg = ...\n",
    "    x = self.classification_head(sequence_avg)\n",
    "    logprobs = F.log_softmax(x, dim=1)\n",
    "\n",
    "    return logprobs\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {},
    "id": "8k09iJGSW2qk"
   },
   "outputs": [],
   "source": [
    "# TODO:to_remove solution\n",
    "class Transformer(nn.Module):\n",
    "  \"\"\" Transformer Encoder network for classification. \"\"\"\n",
    "\n",
    "  def __init__(self, k, heads, depth, seq_length, num_tokens, num_classes):\n",
    "    \"\"\"\n",
    "    Initiates the Transformer Network\n",
    "\n",
    "    Args:\n",
    "      k: Integer\n",
    "        Attention embedding size\n",
    "      heads: Integer\n",
    "        Number of self attention heads\n",
    "      depth: Integer\n",
    "        Number of Transformer Blocks\n",
    "      seq_length: Integer\n",
    "        Length of input sequence\n",
    "      num_tokens: Integer\n",
    "        Size of dictionary\n",
    "      num_classes: Integer\n",
    "        Number of output classes\n",
    "\n",
    "    Returns:\n",
    "      Nothing\n",
    "    \"\"\"\n",
    "    super().__init__()\n",
    "\n",
    "    self.k = k\n",
    "    self.num_tokens = num_tokens\n",
    "    self.token_embedding = nn.Embedding(num_tokens, k)\n",
    "    self.pos_enc = PositionalEncoding(k)\n",
    "\n",
    "    transformer_blocks = []\n",
    "    for i in range(depth):\n",
    "      transformer_blocks.append(TransformerBlock(k=k, heads=heads))\n",
    "\n",
    "    self.transformer_blocks = nn.Sequential(*transformer_blocks)\n",
    "    self.classification_head = nn.Linear(k, num_classes)\n",
    "\n",
    "  def forward(self, x):\n",
    "    \"\"\"\n",
    "    Forward pass for Classification within Transformer network\n",
    "\n",
    "    Args:\n",
    "      x: Tensor\n",
    "        (b, t) sized tensor of tokenized words\n",
    "\n",
    "    Returns:\n",
    "      logprobs: Tensor\n",
    "        Log-probabilities over classes sized (b, c)\n",
    "    \"\"\"\n",
    "    x = self.token_embedding(x) * np.sqrt(self.k)\n",
    "    x = self.pos_enc(x)\n",
    "    x = self.transformer_blocks(x)\n",
    "\n",
    "    sequence_avg = x.mean(dim=1)\n",
    "    x = self.classification_head(sequence_avg)\n",
    "    logprobs = F.log_softmax(x, dim=1)\n",
    "    return logprobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "2in14U9lW2qk"
   },
   "source": [
    "### Training the Transformer\n",
    "\n",
    "Let's now run the Transformer on the Yelp dataset!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {},
    "id": "t4_IqvlfW2qk",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "eb86a22d-29ad-4b29-c746-9071dbaa7355"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Random seed 2021 has been set.\n",
      "Error: DotProductAttention is not implemented.\n"
     ]
    }
   ],
   "source": [
    "def train(model, loss_fn, train_loader,\n",
    "          n_iter=4, learning_rate=1e-4,\n",
    "          test_loader=None, device='cpu',\n",
    "          L2_penalty=0, L1_penalty=0):\n",
    "  \"\"\"\n",
    "  Run gradient descent to opimize parameters of a given network\n",
    "\n",
    "  Args:\n",
    "    net: nn.Module\n",
    "      PyTorch network whose parameters to optimize\n",
    "    loss_fn: nn.Module\n",
    "      Built-in PyTorch loss function to minimize\n",
    "    train_data: Tensor\n",
    "      n_train x n_neurons tensor with neural responses to train on\n",
    "    train_labels: Tensor\n",
    "      n_train x 1 tensor with orientations of the stimuli corresponding to each row of train_data\n",
    "    n_iter: Integer, optional\n",
    "      Number of iterations of gradient descent to run\n",
    "    learning_rate: Float, optional\n",
    "      Learning rate to use for gradient descent\n",
    "    test_data: Tensor, optional\n",
    "      n_test x n_neurons tensor with neural responses to test on\n",
    "    test_labels: Tensor, optional\n",
    "      n_test x 1 tensor with orientations of the stimuli corresponding to each row of test_data\n",
    "    L2_penalty: Float, optional\n",
    "      l2 penalty regularizer coefficient\n",
    "    L1_penalty: Float, optional\n",
    "      l1 penalty regularizer coefficient\n",
    "\n",
    "  Returns:\n",
    "    train_loss/test_loss: List\n",
    "      Training/Test loss over iterations\n",
    "  \"\"\"\n",
    "\n",
    "  # Initialize PyTorch Adam optimizer\n",
    "  optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "  # Placeholder to save the loss at each iteration\n",
    "  train_loss = []\n",
    "  test_loss = []\n",
    "\n",
    "  # Loop over epochs (cf. appendix)\n",
    "  for iter in range(n_iter):\n",
    "    iter_train_loss = []\n",
    "    for i, batch in tqdm(enumerate(train_loader)):\n",
    "      # compute network output from inputs in train_data\n",
    "      out = model(batch['input_ids'].to(device))\n",
    "      loss = loss_fn(out, batch['label'].to(device))\n",
    "\n",
    "      # Clear previous gradients\n",
    "      optimizer.zero_grad()\n",
    "\n",
    "      # Compute gradients\n",
    "      loss.backward()\n",
    "\n",
    "      # Update weights\n",
    "      optimizer.step()\n",
    "\n",
    "      # Store current value of loss\n",
    "      iter_train_loss.append(loss.item())  # .item() needed to transform the tensor output of loss_fn to a scalar\n",
    "      if i % 50 == 0:\n",
    "        print(f'[Batch {i}]: train_loss: {loss.item()}')\n",
    "    train_loss.append(statistics.mean(iter_train_loss))\n",
    "\n",
    "    # Track progress\n",
    "    if True:  # (iter + 1) % (n_iter // 5) == 0:\n",
    "\n",
    "      if test_loader is not None:\n",
    "        print('Running Test loop')\n",
    "        iter_loss_test = []\n",
    "        for j, test_batch in enumerate(test_loader):\n",
    "\n",
    "          out_test = model(test_batch['input_ids'].to(device))\n",
    "          loss_test = loss_fn(out_test, test_batch['label'].to(device))\n",
    "          iter_loss_test.append(loss_test.item())\n",
    "\n",
    "        test_loss.append(statistics.mean(iter_loss_test))\n",
    "\n",
    "      if test_loader is None:\n",
    "        print(f'iteration {iter + 1}/{n_iter} | train loss: {loss.item():.3f}')\n",
    "      else:\n",
    "        print(f'iteration {iter + 1}/{n_iter} | train loss: {loss.item():.3f} | test_loss: {loss_test.item():.3f}')\n",
    "\n",
    "  if test_loader is None:\n",
    "    return train_loss\n",
    "  else:\n",
    "    return train_loss, test_loss\n",
    "\n",
    "\n",
    "try:\n",
    "  # Set random seeds for reproducibility\n",
    "  set_seed(seed=SEED)\n",
    "\n",
    "  # Initialize network with embedding size 128, 8 attention heads, and 3 layers\n",
    "  model = Transformer(128, 8, 3, max_len, vocab_size, num_classes).to(DEVICE)\n",
    "\n",
    "  # Initialize built-in PyTorch Negative Log Likelihood loss function\n",
    "  loss_fn = F.nll_loss\n",
    "\n",
    "  # Run only on GPU, unless take a lot of time!\n",
    "  if DEVICE != 'cpu':\n",
    "    train_loss, test_loss = train(model,\n",
    "                                  loss_fn,\n",
    "                                  train_loader,\n",
    "                                  test_loader=test_loader,\n",
    "                                  device=DEVICE)\n",
    "except NameError:\n",
    "    print(\"Error: DotProductAttention is not implemented.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "ML8HTyvjW2ql"
   },
   "source": [
    "### Prediction\n",
    "\n",
    "Check out the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {},
    "id": "m1p4z7ydW2ql",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "a1aab00d-a06f-4530-c061-09db9e6d3795"
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Error: Model no Initialized. DotProductAttention is not implemented.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  with torch.no_grad():\n",
    "    # Batch 1 contains all the tokenized text for the 1st batch of the test loader\n",
    "    pred_batch = model(batch1['input_ids'].to(DEVICE))\n",
    "    # Predicting the label for the text\n",
    "    print(\"The yelp review is  \" + str(pred_text))\n",
    "    predicted_label28 = np.argmax(pred_batch[28].cpu())\n",
    "    print()\n",
    "    print(\"The Predicted Rating is  \" + str(predicted_label28.item()) + \" and the Actual Rating was  \" + str(actual_label))\n",
    "except NameError:\n",
    "    print(\"Error: Model no Initialized. DotProductAttention is not implemented.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Activity 7\n",
    "Compute the predicted rating with the previous code and submit your result on [AhaSlides](https://ahaslides.com/J43GU)."
   ],
   "metadata": {
    "id": "AIRpwCeaVUhM"
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "tiT52bF2W2ql"
   },
   "source": [
    "---\n",
    "# Section 7: Transformers beyond Language models\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {},
    "id": "NNyDqQbmW2ql"
   },
   "source": [
    "Transformers were originally introduced for language tasks, but since then, transformers have achieved State-of-the-Art performance for many different applications, here we discuss some of them:\n",
    "\n",
    "* Computer Vision - Vision Transformers: [ViT](https://arxiv.org/abs/2010.11929)\n",
    "* Art & Creativity: [OpenAI Dall-E 2](https://openai.com/dall-e-2/)* and [Google Parti](https://parti.research.google/)\n",
    "* Vision & Language: [DeepMind Flamingo](https://www.deepmind.com/blog/tackling-multiple-tasks-with-a-single-visual-language-model)\n",
    "* 3D Scene Representations: [NeRF](https://www.matthewtancik.com/nerf)\n",
    "* Speech: [FAIR Wav2Vec 2.0](https://arxiv.org/pdf/2006.11477.pdf)\n",
    "* Generalist Agent: [DeepMind Gato](https://www.deepmind.com/publications/a-generalist-agent)\n",
    "\n",
    "**Note:** Dall-E was a transformer-based model but Dall-E 2 has moved towards Diffusion and uses transformers for specifics such as diffusion priors."
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "---\n",
    "# Acknowledgement\n",
    "Material is based on [Neuromatch](https://github.com/NeuromatchAcademy/course-content-dl/tree/main/tutorials)"
   ],
   "metadata": {
    "id": "H9CtNAZuknaU"
   }
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernel": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "61b90e4b4fe34fa9b848e1de41125f36": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0679cab88fe549acac005be4c31e750b",
       "IPY_MODEL_5bbafafbd02f4574ae6a3e5ddb7ccf00",
       "IPY_MODEL_97547b09b19b4bd097d537e78451fe8b"
      ],
      "layout": "IPY_MODEL_df97936762764369becaaa439d9df599"
     }
    },
    "0679cab88fe549acac005be4c31e750b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f08f83724e9e44beaed19e44418042b7",
      "placeholder": "",
      "style": "IPY_MODEL_28b3888bdb2d4997851d4214636e65fd",
      "value": "Map:100%"
     }
    },
    "5bbafafbd02f4574ae6a3e5ddb7ccf00": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_c590b8837b6248b787878a9cdf7ad7bc",
      "max": 5000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_62f3def14dfe41cbb2de9d269548571a",
      "value": 5000
     }
    },
    "97547b09b19b4bd097d537e78451fe8b": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "model_module_version": "1.5.0",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5b68681e2bb54f9b9b98fa28dc1b2597",
      "placeholder": "",
      "style": "IPY_MODEL_3ecc7bb2d82041f7a32296808b9e01e6",
      "value": "5000/5000[00:05&lt;00:00,926.18examples/s]"
     }
    },
    "df97936762764369becaaa439d9df599": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f08f83724e9e44beaed19e44418042b7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "28b3888bdb2d4997851d4214636e65fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c590b8837b6248b787878a9cdf7ad7bc": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "62f3def14dfe41cbb2de9d269548571a": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5b68681e2bb54f9b9b98fa28dc1b2597": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "model_module_version": "1.2.0",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3ecc7bb2d82041f7a32296808b9e01e6": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "model_module_version": "1.5.0",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
